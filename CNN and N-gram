import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Dropout

# Load the Amazon review dataset
data = pd.read_csv("reviews.csv")

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(data['review'], data['label'], test_size=0.2)

# Create a CountVectorizer to extract n-grams
vectorizer = CountVectorizer(ngram_range=(1,3), max_features=5000)

# Fit the vectorizer on the training data and transform it
X_train = vectorizer.fit_transform(X_train)

# Transform the test data using the fitted vectorizer
X_test = vectorizer.transform(X_test)

# Convert the n-grams to a 2D array
X_train = X_train.toarray()
X_test = X_test.toarray()

# Reshape the data for use in a CNN
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)

# Define the model
model = Sequential()
model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(units=250, activation='relu'
# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Fit the model on the training data
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))
# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_test)
print("Test loss:", loss)
print("Test accuracy:", accuracy)
# Make predictions on new data
predictions = model.predict(X_new)
